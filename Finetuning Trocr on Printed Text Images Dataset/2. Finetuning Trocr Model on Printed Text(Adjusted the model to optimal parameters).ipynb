{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers==4.45.0\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q jiwer\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q -U accelerate\n",
    "\n",
    "!pip install -q matplotlib\n",
    "!pip install -q protobuf==3.20.1\n",
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator,\n",
    "    get_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_plot = False\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "\n",
    "bold = f\"\\033[1m\"\n",
    "reset = f\"\\033[0m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed setting for reproducibility\n",
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "def download_and_unzip(url, save_path):\n",
    "    print(f\"Downloading and extracting assets....\", end=\"\")\n",
    "\n",
    "    urlretrieve(url, save_path)\n",
    "\n",
    "    try:\n",
    "        with ZipFile(save_path) as z:\n",
    "            z.extractall(os.path.split(save_path)[0])\n",
    "\n",
    "        print(\"Done\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nInvalid file.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = r\"https://www.dropbox.com/scl/fi/vyvr7jbdvu8o174mbqgde/scut_data.zip?rlkey=fs8axkpxunwu6if9a2su71kxs&dl=1\"\n",
    "asset_zip_path = os.path.join(os.getcwd(), \"scut_data.zip\")\n",
    "if not os.path.exists(asset_zip_path):\n",
    "    download_and_unzip(URL, asset_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:    int = 48\n",
    "    EPOCHS:        int = 35\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "    WARMUP_STEPS: int = 2000\n",
    "    LR_MAX: float = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    DATA_ROOT:     str = 'scut_data'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "\n",
    "# Augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and datasets\n",
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_df = pd.read_fwf(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'scut_train.txt'), header=None\n",
    ")\n",
    "train_df.rename(columns={0: 'file_name', 1: 'text'}, inplace=True)\n",
    "test_df = pd.read_fwf(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'scut_test.txt'), header=None\n",
    ")\n",
    "test_df.rename(columns={0: 'file_name', 1: 'text'}, inplace=True)\n",
    "\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'scut_train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'scut_test/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=TrainingConfig.WARMUP_STEPS,\n",
    "    num_training_steps=TrainingConfig.EPOCHS * len(train_dataset) // TrainingConfig.BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch size\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    output_dir='seq2seq_model_printed/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='tensorboard',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS,\n",
    "    learning_rate=TrainingConfig.LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, VisionEncoderDecoderModel\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CER metric\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute CER on the validation set\n",
    "def compute_cer(model, tokenizer, valid_dataset):\n",
    "    model.eval()\n",
    "    cer_score = 0\n",
    "    total_samples = len(valid_dataset)\n",
    "\n",
    "    # Loop through the validation set\n",
    "    for i in tqdm(range(total_samples)):\n",
    "        encoding = valid_dataset[i]\n",
    "        pixel_values = encoding[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "        labels = encoding[\"labels\"].unsqueeze(0).to(device)\n",
    "\n",
    "        # Get predictions from the model\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        pred_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        true_text = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "\n",
    "        # Compute CER\n",
    "        cer_score += cer_metric.compute(predictions=[pred_text], references=[true_text])\n",
    "\n",
    "    avg_cer = cer_score / total_samples\n",
    "    return avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CER for the validation set\n",
    "validation_cer = compute_cer(model, tokenizer, valid_dataset)\n",
    "print(f\"Validation CER: {validation_cer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
