{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets jiwer evaluate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "df = pd.read_fwf('/teamspace/studios/this_studio/IAM/gt_test.txt', header=None)\n",
    "df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
    "del df[2]\n",
    "df['file_name'] = df['file_name'].apply(lambda x: x + 'g' if x.endswith('jp') else x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        labels = self.processor.tokenizer(text,\n",
    "                                          padding=\"max_length\",\n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        return {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load processor and datasets\n",
    "from transformers import TrOCRProcessor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='/teamspace/studios/this_studio/IAM/image/',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='/teamspace/studios/this_studio/IAM/image/',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['file_name'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Increased batch size\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load model\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set special tokens and model parameters\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define optimizer, scheduler, and metrics\n",
    "from transformers import AdamW, get_scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_training_steps = len(train_dataloader) * 10  # 10 epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)  # 10% of total training steps as warmup\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=num_warmup_steps, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluate import load\n",
    "cer_metric = load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return cer_metric.compute(predictions=pred_str, references=label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Add data augmentation\n",
    "from torchvision.transforms import Compose, RandomRotation, ColorJitter, ToTensor\n",
    "\n",
    "augmentation = Compose([\n",
    "    RandomRotation(degrees=10),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class with augmentation\n",
    "class IAMAugmentedDataset(IAMDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        image = augmentation(image)  # Apply augmentation\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        labels = self.processor.tokenizer(text,\n",
    "                                          padding=\"max_length\",\n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        return {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "train_dataset = IAMAugmentedDataset(root_dir='/teamspace/studios/this_studio/IAM/image/',\n",
    "                                    df=train_df,\n",
    "                                    processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train the model\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "\n",
    "scaler = amp.GradScaler()  # Initialize scaler for mixed precision\n",
    "\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        \n",
    "        with amp.autocast():  # Mixed precision\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss / len(train_dataloader))\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer\n",
    "\n",
    "    print(f\"Validation CER after epoch {epoch}:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "# Step 9: Save the model\n",
    "model.save_pretrained(\"improved_trocr_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Validation CER : ~5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Prepare the test dataloader\n",
    "test_dataloader = DataLoader(eval_dataset, batch_size=4)\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# List to store predictions and ground truth\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "# Iterate through the test data\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # Move the batch to device\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(batch[\"pixel_values\"])\n",
    "\n",
    "        # Decode predictions\n",
    "        pred_str = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Handle the -100 padding tokens in labels\n",
    "        labels = batch[\"labels\"]\n",
    "        labels[labels == -100] = processor.tokenizer.pad_token_id  # replace -100 with pad_token_id\n",
    "        \n",
    "        # Decode labels\n",
    "        label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(pred_str)\n",
    "        ground_truths.extend(label_str)\n",
    "\n",
    "# Displaying some results\n",
    "for i in range(5):  # Displaying 5 predictions\n",
    "    print(f\"Ground Truth: {ground_truths[i]}\")\n",
    "    print(f\"Prediction:   {predictions[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Optionally, calculate CER on the whole test set\n",
    "from evaluate import load\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "# Compute CER for the entire test set\n",
    "test_cer = cer_metric.compute(predictions=predictions, references=ground_truths)\n",
    "print(f\"Test CER: {test_cer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CER : ~5.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Function to predict text from a given image\n",
    "def predict_from_image(image_path, model, processor):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Preprocess the image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Run the image through the model to generate predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values.to(model.device))\n",
    "\n",
    "    # Decode the predicted ids to text\n",
    "    predicted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "# Test the function with a test image\n",
    "image_path = '/teamspace/studios/this_studio/Screenshot 2024-12-22 132529.png'  # Specify the path to your test image\n",
    "predicted_text = predict_from_image(image_path, model, processor)\n",
    "\n",
    "# Display the predicted text\n",
    "print(\"Predicted Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def predict_from_custom_image(image_path, model, processor, device):\n",
    "    \"\"\"\n",
    "    Predict the text from a custom image using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "    - model (PreTrainedModel): TrOCR model.\n",
    "    - processor (TrOCRProcessor): Processor for TrOCR.\n",
    "    - device (torch.device): Device to run the model on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess the image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    # Run the model to generate predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "    \n",
    "    # Decode the output to text\n",
    "    decoded_output = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Print the predicted text\n",
    "    print(f\"Predicted Text: {decoded_output[0]}\")\n",
    "\n",
    "# Example usage:\n",
    "image_path = '/teamspace/studios/this_studio/WhatsApp Image 2024-12-29 at 19.00.00_6f4550fl.jpg'  # Replace with your image path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to GPU if available\n",
    "\n",
    "predict_from_custom_image(image_path, model, processor, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
